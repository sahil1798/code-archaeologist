# Oumi Training Configuration for Code Archaeologist

model:
  base_model: "meta-llama/Llama-3.2-3B-Instruct"
  load_in_4bit: true  # Use quantization for lower memory
  
training:
  method: "lora"  # Low-Rank Adaptation
  lora_r: 64
  lora_alpha: 128
  lora_dropout: 0.05
  
  num_epochs: 3
  batch_size: 2
  gradient_accumulation_steps: 4
  learning_rate: 2.0e-4
  
  optimizer: "adamw"
  lr_scheduler_type: "cosine"
  warmup_ratio: 0.03
  
  max_seq_length: 2048

data:
  train_file: "./training-data.jsonl"
  data_format: "conversation"

output:
  output_dir: "./output/code-archaeologist-model"
  save_steps: 100
  logging_steps: 10
