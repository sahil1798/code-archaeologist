model:
 base_model: "meta-llama/Llama-3.2-3B"
 adapter_type: "lora"
 lora_r: 16
 lora_alpha: 32
 lora_dropout: 0.05
training:
 epochs: 3
 batch_size: 4
 learning_rate: 2e-4
 warmup_steps: 100
 max_length: 2048
 gradient_accumulation_steps: 4
data:
 train_file: "data/git_context_training.jsonl"
 validation_file: "data/git_context_validation.jsonl"
 format: "instruction"
output:
 model_dir: "models/code-archaeologist-v1"
 save_steps: 500
